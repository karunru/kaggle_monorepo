#!/usr/bin/env python3
"""
Collision Detection Dataset Detailed Visualizer and Analyzer

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€KDXF Collision Detection ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°ãªå¯è¦–åŒ–ã¨ç‰¹å¾´é‡åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ç›¸é–¢é–¢ä¿‚ã€è¡çªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import logging
import sys
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parents[3]
sys.path.append(str(project_root))

from codes.src.utils.logger import create_logger


class CollisionDataVisualizer:
    """Collision Detection ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str | Path, results_file: str | Path = None):
        """
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            results_file: åŸºæœ¬è§£æçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.data_dir = Path(data_dir)
        self.train_path = self.data_dir / "train_data.csv"
        self.test_path = self.data_dir / "test_data.csv"
        self.logger = create_logger(__name__)
        
        # åŸºæœ¬è§£æçµæœã®èª­ã¿è¾¼ã¿
        if results_file and Path(results_file).exists():
            with open(results_file, 'r') as f:
                self.basic_results = json.load(f)
        else:
            self.basic_results = {}
        
        # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
        plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')
        sns.set_palette("husl")
    
    def analyze_sensor_types(self) -> Dict[str, Any]:
        """ã‚»ãƒ³ã‚µãƒ¼ç¨®åˆ¥ã®åˆ†æ"""
        self.logger.info("Analyzing sensor types")
        
        # åŸºæœ¬çµæœã‹ã‚‰ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å–å¾—
        train_columns = self.basic_results.get("train_structure", {}).get("columns", [])
        
        sensor_types = {
            "acceleration": [],
            "rotation": [],
            "thermal": [],
            "time_of_flight": [],
            "metadata": [],
            "other": []
        }
        
        for col in train_columns:
            col_lower = col.lower()
            if "acc" in col_lower:
                sensor_types["acceleration"].append(col)
            elif "rot" in col_lower:
                sensor_types["rotation"].append(col)
            elif "thm" in col_lower:
                sensor_types["thermal"].append(col)
            elif "tof" in col_lower:
                sensor_types["time_of_flight"].append(col)
            elif col in ["row_id", "sequence_id", "risk_score"]:
                sensor_types["metadata"].append(col)
            else:
                sensor_types["other"].append(col)
        
        # çµ±è¨ˆæƒ…å ±
        sensor_stats = {}
        for sensor_type, columns in sensor_types.items():
            sensor_stats[sensor_type] = {
                "count": len(columns),
                "sample_columns": columns[:5] if columns else []
            }
        
        return {
            "sensor_types": sensor_types,
            "sensor_stats": sensor_stats,
            "total_sensors": sum(len(cols) for cols in sensor_types.values())
        }
    
    def analyze_target_distribution(self, sample_size: int = 50000) -> Dict[str, Any]:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆrisk_scoreï¼‰ã®åˆ†å¸ƒåˆ†æ"""
        self.logger.info("Analyzing target distribution")
        
        try:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§åˆ†æ
            df = pd.read_csv(self.train_path, nrows=sample_size)
            
            if 'risk_score' not in df.columns:
                return {"error": "risk_score column not found"}
            
            # åŸºæœ¬çµ±è¨ˆ
            risk_stats = {
                "count": len(df),
                "mean": float(df['risk_score'].mean()),
                "std": float(df['risk_score'].std()),
                "min": float(df['risk_score'].min()),
                "max": float(df['risk_score'].max()),
                "median": float(df['risk_score'].median()),
                "unique_values": int(df['risk_score'].nunique())
            }\n            \n            # åˆ†å¸ƒã®è©³ç´°\n            value_counts = df['risk_score'].value_counts().sort_index()\n            distribution = {\n                \"value_counts\": {str(k): int(v) for k, v in value_counts.items()},\n                \"class_balance\": {\n                    \"positive_ratio\": float((df['risk_score'] > 0).mean()),\n                    \"zero_ratio\": float((df['risk_score'] == 0).mean()),\n                    \"negative_ratio\": float((df['risk_score'] < 0).mean())\n                }\n            }\n            \n            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ¥ã®åˆ†æ\n            if 'sequence_id' in df.columns:\n                seq_stats = df.groupby('sequence_id')['risk_score'].agg([\n                    'mean', 'std', 'min', 'max', 'count'\n                ]).round(4)\n                \n                sequence_analysis = {\n                    \"total_sequences\": len(seq_stats),\n                    \"avg_sequence_length\": float(seq_stats['count'].mean()),\n                    \"sequence_risk_variation\": {\n                        \"high_risk_sequences\": int((seq_stats['mean'] > 0.5).sum()),\n                        \"medium_risk_sequences\": int(((seq_stats['mean'] >= 0) & (seq_stats['mean'] <= 0.5)).sum()),\n                        \"low_risk_sequences\": int((seq_stats['mean'] < 0).sum())\n                    }\n                }\n            else:\n                sequence_analysis = {\"error\": \"sequence_id not found\"}\n            \n            return {\n                \"risk_stats\": risk_stats,\n                \"distribution\": distribution,\n                \"sequence_analysis\": sequence_analysis,\n                \"sample_size\": len(df)\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Error analyzing target distribution: {e}\")\n            return {\"error\": str(e)}\n    \n    def analyze_sensor_data_patterns(self, sample_size: int = 10000) -> Dict[str, Any]:\n        \"\"\"ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\"\"\"\n        self.logger.info(\"Analyzing sensor data patterns\")\n        \n        try:\n            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n            df = pd.read_csv(self.train_path, nrows=sample_size)\n            \n            # ã‚»ãƒ³ã‚µãƒ¼ç¨®åˆ¥ã‚’å–å¾—\n            sensor_analysis = self.analyze_sensor_types()\n            sensor_types = sensor_analysis[\"sensor_types\"]\n            \n            pattern_results = {}\n            \n            # å„ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—ã®åˆ†æ\n            for sensor_type, columns in sensor_types.items():\n                if not columns or sensor_type == \"metadata\":\n                    continue\n                    \n                self.logger.info(f\"Analyzing {sensor_type} sensors\")\n                \n                # æœ€åˆã®ã„ãã¤ã‹ã®ã‚«ãƒ©ãƒ ã‚’åˆ†æ\n                sample_columns = columns[:5]\n                sensor_data = df[sample_columns]\n                \n                # åŸºæœ¬çµ±è¨ˆ\n                basic_stats = {}\n                for col in sample_columns:\n                    basic_stats[col] = {\n                        \"mean\": float(sensor_data[col].mean()),\n                        \"std\": float(sensor_data[col].std()),\n                        \"min\": float(sensor_data[col].min()),\n                        \"max\": float(sensor_data[col].max()),\n                        \"missing_count\": int(sensor_data[col].isnull().sum()),\n                        \"zero_count\": int((sensor_data[col] == 0).sum()),\n                        \"negative_one_count\": int((sensor_data[col] == -1).sum())\n                    }\n                \n                # ç›¸é–¢åˆ†æ\n                if len(sample_columns) > 1:\n                    correlation_matrix = sensor_data.corr()\n                    # ä¸Šä¸‰è§’è¡Œåˆ—ã®ç›¸é–¢å€¤ã‚’å–å¾—\n                    correlations = []\n                    for i in range(len(correlation_matrix)):\n                        for j in range(i+1, len(correlation_matrix)):\n                            correlations.append({\n                                \"col1\": correlation_matrix.index[i],\n                                \"col2\": correlation_matrix.columns[j],\n                                \"correlation\": float(correlation_matrix.iloc[i, j])\n                            })\n                    \n                    correlation_stats = {\n                        \"avg_correlation\": float(np.mean([c[\"correlation\"] for c in correlations if not np.isnan(c[\"correlation\"])])),\n                        \"max_correlation\": max([c[\"correlation\"] for c in correlations if not np.isnan(c[\"correlation\"])]),\n                        \"min_correlation\": min([c[\"correlation\"] for c in correlations if not np.isnan(c[\"correlation\"])]),\n                        \"high_corr_pairs\": [c for c in correlations if abs(c[\"correlation\"]) > 0.8 and not np.isnan(c[\"correlation\"])]\n                    }\n                else:\n                    correlation_stats = {\"note\": \"Single column, no correlation analysis\"}\n                \n                pattern_results[sensor_type] = {\n                    \"column_count\": len(columns),\n                    \"analyzed_columns\": sample_columns,\n                    \"basic_stats\": basic_stats,\n                    \"correlation_analysis\": correlation_stats\n                }\n            \n            return {\n                \"pattern_analysis\": pattern_results,\n                \"sample_size\": sample_size,\n                \"total_columns_analyzed\": sum(len(result.get(\"analyzed_columns\", [])) for result in pattern_results.values())\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Error analyzing sensor patterns: {e}\")\n            return {\"error\": str(e)}\n    \n    def analyze_sequence_characteristics(self, sample_size: int = 20000) -> Dict[str, Any]:\n        \"\"\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç‰¹æ€§ã®åˆ†æ\"\"\"\n        self.logger.info(\"Analyzing sequence characteristics\")\n        \n        try:\n            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n            df = pd.read_csv(self.train_path, nrows=sample_size)\n            \n            if 'sequence_id' not in df.columns:\n                return {\"error\": \"sequence_id column not found\"}\n            \n            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã”ã¨ã®åŸºæœ¬çµ±è¨ˆ\n            seq_stats = df.groupby('sequence_id').agg({\n                'row_id': 'count',  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n                'risk_score': ['mean', 'std', 'min', 'max'] if 'risk_score' in df.columns else 'count'\n            }).round(4)\n            \n            # ã‚«ãƒ©ãƒ åã‚’å¹³å¦åŒ–\n            if 'risk_score' in df.columns:\n                seq_stats.columns = ['sequence_length', 'risk_mean', 'risk_std', 'risk_min', 'risk_max']\n            else:\n                seq_stats.columns = ['sequence_length']\n            \n            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†æ\n            length_analysis = {\n                \"total_sequences\": len(seq_stats),\n                \"avg_length\": float(seq_stats['sequence_length'].mean()),\n                \"median_length\": float(seq_stats['sequence_length'].median()),\n                \"min_length\": int(seq_stats['sequence_length'].min()),\n                \"max_length\": int(seq_stats['sequence_length'].max()),\n                \"std_length\": float(seq_stats['sequence_length'].std()),\n                \"length_distribution\": seq_stats['sequence_length'].value_counts().head(10).to_dict()\n            }\n            \n            # ãƒªã‚¹ã‚¯åˆ†æï¼ˆrisk_scoreãŒã‚ã‚‹å ´åˆï¼‰\n            if 'risk_score' in df.columns:\n                risk_analysis = {\n                    \"sequences_with_risk\": int((seq_stats['risk_max'] > 0).sum()),\n                    \"sequences_without_risk\": int((seq_stats['risk_max'] <= 0).sum()),\n                    \"high_risk_sequences\": int((seq_stats['risk_mean'] > 0.5).sum()),\n                    \"avg_sequence_risk\": float(seq_stats['risk_mean'].mean()),\n                    \"risk_variability_by_sequence\": float(seq_stats['risk_std'].mean())\n                }\n            else:\n                risk_analysis = {\"note\": \"No risk_score column found\"}\n            \n            # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæœ€åˆã®ã„ãã¤ã‹ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è©³ç´°åˆ†æï¼‰\n            sample_sequences = df['sequence_id'].unique()[:5]\n            sequence_patterns = {}\n            \n            for seq_id in sample_sequences:\n                seq_data = df[df['sequence_id'] == seq_id]\n                \n                # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å¤‰åŒ–ç‡åˆ†æ\n                sensor_columns = [col for col in df.columns \n                                if any(sensor in col.lower() for sensor in ['acc', 'rot', 'thm', 'tof'])]\n                \n                if sensor_columns:\n                    # æœ€åˆã®ã„ãã¤ã‹ã®ã‚»ãƒ³ã‚µãƒ¼ã®ã¿åˆ†æ\n                    sample_sensors = sensor_columns[:10]\n                    sensor_data = seq_data[sample_sensors]\n                    \n                    # å¤‰åŒ–ç‡è¨ˆç®—\n                    change_rates = sensor_data.diff().abs().mean()\n                    \n                    sequence_patterns[str(seq_id)] = {\n                        \"length\": len(seq_data),\n                        \"risk_pattern\": seq_data['risk_score'].tolist()[:10] if 'risk_score' in seq_data.columns else [],\n                        \"sensor_change_rates\": {col: float(rate) for col, rate in change_rates.items() if not pd.isna(rate)},\n                        \"avg_change_rate\": float(change_rates.mean())\n                    }\n            \n            return {\n                \"length_analysis\": length_analysis,\n                \"risk_analysis\": risk_analysis,\n                \"sequence_patterns\": sequence_patterns,\n                \"sample_size\": sample_size\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Error analyzing sequence characteristics: {e}\")\n            return {\"error\": str(e)}\n    \n    def compare_with_cmi_dataset(self) -> Dict[str, Any]:\n        \"\"\"CMI Detect Behavior ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã®æ¯”è¼ƒåˆ†æ\"\"\"\n        self.logger.info(\"Comparing with CMI dataset characteristics\")\n        \n        # Collision Detection ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å¾´\n        collision_features = {\n            \"dataset_size\": {\n                \"train_rows\": self.basic_results.get(\"train_structure\", {}).get(\"total_rows\", 0),\n                \"test_rows\": self.basic_results.get(\"test_structure\", {}).get(\"total_rows\", 0),\n                \"train_cols\": self.basic_results.get(\"train_structure\", {}).get(\"total_columns\", 0),\n                \"test_cols\": self.basic_results.get(\"test_structure\", {}).get(\"total_columns\", 0)\n            },\n            \"data_characteristics\": {\n                \"has_sequence_id\": \"sequence_id\" in self.basic_results.get(\"train_structure\", {}).get(\"columns\", []),\n                \"has_continuous_target\": \"risk_score\" in self.basic_results.get(\"train_structure\", {}).get(\"columns\", []),\n                \"sensor_types\": [\"acceleration\", \"rotation\", \"thermal\", \"time_of_flight\"],\n                \"multi_user_sensors\": True,  # userA_ prefix suggests multi-user data\n                \"time_series_nature\": \"sequence_based\"\n            }\n        }\n        \n        # CMI Detect Behavior ã®ä¸€èˆ¬çš„ãªç‰¹å¾´ï¼ˆæ¨å®šï¼‰\n        cmi_estimated_features = {\n            \"dataset_size\": {\n                \"note\": \"CMI dataset characteristics would need to be analyzed separately\"\n            },\n            \"data_characteristics\": {\n                \"typical_cmi_features\": [\"sequence_based_time_series\", \"behavioral_patterns\", \"sensor_data\"],\n                \"expected_differences\": [\n                    \"Different sensor types\",\n                    \"Different target variable (behavior vs collision risk)\",\n                    \"Different temporal patterns\",\n                    \"Different feature engineering opportunities\"\n                ]\n            }\n        }\n        \n        # æ¯”è¼ƒãƒã‚¤ãƒ³ãƒˆ\n        comparison_points = {\n            \"similarities\": [\n                \"Both are sequence-based time series datasets\",\n                \"Both involve sensor data analysis\",\n                \"Both require temporal pattern recognition\",\n                \"Both can benefit from similar feature engineering techniques\"\n            ],\n            \"differences\": [\n                \"Target variable: collision risk vs behavior detection\",\n                \"Sensor types: collision sensors vs behavior sensors\",\n                \"Temporal dynamics: collision events vs behavioral patterns\",\n                \"Feature complexity: multi-user collision detection vs individual behavior\"\n            ],\n            \"transferable_techniques\": [\n                \"Time series feature engineering\",\n                \"Sequence-based deep learning models (LSTM, Transformers)\",\n                \"Sensor data preprocessing techniques\",\n                \"Cross-validation strategies for time series\",\n                \"Ensemble methods\"\n            ],\n            \"unique_challenges\": {\n                \"collision_detection\": [\n                    \"Multi-user interaction patterns\",\n                    \"Collision risk assessment\",\n                    \"Real-time prediction requirements\",\n                    \"Safety-critical decision making\"\n                ],\n                \"behavior_detection\": [\n                    \"Long-term behavioral pattern recognition\",\n                    \"Individual variation modeling\",\n                    \"Contextual behavior interpretation\"\n                ]\n            }\n        }\n        \n        return {\n            \"collision_dataset_features\": collision_features,\n            \"cmi_estimated_features\": cmi_estimated_features,\n            \"comparison_analysis\": comparison_points\n        }\n    \n    def generate_comprehensive_report(self) -> Dict[str, Any]:\n        \"\"\"åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\"\"\"\n        self.logger.info(\"Generating comprehensive analysis report\")\n        \n        report = {\n            \"dataset_overview\": {\n                \"name\": \"KDXF Collision Detection Dataset\",\n                \"basic_info\": self.basic_results.get(\"file_info\", {}),\n                \"structure_info\": {\n                    \"train\": self.basic_results.get(\"train_structure\", {}),\n                    \"test\": self.basic_results.get(\"test_structure\", {})\n                }\n            }\n        }\n        \n        # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ\n        try:\n            report[\"sensor_analysis\"] = self.analyze_sensor_types()\n            report[\"target_analysis\"] = self.analyze_target_distribution()\n            report[\"pattern_analysis\"] = self.analyze_sensor_data_patterns()\n            report[\"sequence_analysis\"] = self.analyze_sequence_characteristics()\n            report[\"cmi_comparison\"] = self.compare_with_cmi_dataset()\n            \n        except Exception as e:\n            self.logger.error(f\"Error in comprehensive analysis: {e}\")\n            report[\"analysis_error\"] = str(e)\n        \n        return report\n\n\ndef main():\n    \"\"\"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°\"\"\"\n    \n    # ãƒ‘ã‚¹è¨­å®š\n    project_root = Path(__file__).parents[3]\n    data_dir = project_root / \"public_datasets\" / \"kdxf-collisiondetect\"\n    results_file = project_root / \"outputs\" / \"claude\" / \"collision_dataset_structure_analysis.json\"\n    output_dir = project_root / \"outputs\" / \"claude\"\n    \n    # è©³ç´°è§£æå®Ÿè¡Œ\n    visualizer = CollisionDataVisualizer(data_dir, results_file)\n    comprehensive_report = visualizer.generate_comprehensive_report()\n    \n    # çµæœä¿å­˜\n    output_path = output_dir / \"collision_dataset_detailed_analysis.json\"\n    \n    def convert_numpy_types(obj):\n        \"\"\"Numpyãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’PythonåŸºæœ¬å‹ã«å¤‰æ›\"\"\"\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, dict):\n            return {key: convert_numpy_types(value) for key, value in obj.items()}\n        elif isinstance(obj, list):\n            return [convert_numpy_types(item) for item in obj]\n        else:\n            return obj\n    \n    report_converted = convert_numpy_types(comprehensive_report)\n    \n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(report_converted, f, ensure_ascii=False, indent=2)\n    \n    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n    print(\"=\"*80)\n    print(\"COLLISION DETECTION DATASET - DETAILED ANALYSIS RESULTS\")\n    print(\"=\"*80)\n    \n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸºæœ¬æƒ…å ±\n    if \"dataset_overview\" in comprehensive_report:\n        overview = comprehensive_report[\"dataset_overview\"]\n        basic_info = overview.get(\"basic_info\", {})\n        \n        print(\"\\nğŸ—‚ï¸  DATASET OVERVIEW:\")\n        for dataset in [\"train\", \"test\"]:\n            if dataset in basic_info:\n                info = basic_info[dataset]\n                if info.get(\"exists\"):\n                    print(f\"  {dataset.upper()}: {info['size_mb']:.1f} MB\")\n    \n    # ã‚»ãƒ³ã‚µãƒ¼åˆ†æçµæœ\n    if \"sensor_analysis\" in comprehensive_report:\n        sensor_info = comprehensive_report[\"sensor_analysis\"]\n        sensor_stats = sensor_info.get(\"sensor_stats\", {})\n        \n        print(\"\\nğŸ›ï¸  SENSOR TYPES:\")\n        for sensor_type, stats in sensor_stats.items():\n            if stats[\"count\"] > 0:\n                print(f\"  {sensor_type.title()}: {stats['count']} columns\")\n                if stats[\"sample_columns\"]:\n                    print(f\"    Sample: {', '.join(stats['sample_columns'][:3])}\")\n    \n    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†æçµæœ\n    if \"target_analysis\" in comprehensive_report and \"error\" not in comprehensive_report[\"target_analysis\"]:\n        target_info = comprehensive_report[\"target_analysis\"]\n        risk_stats = target_info.get(\"risk_stats\", {})\n        \n        print(\"\\nğŸ¯ TARGET VARIABLE (risk_score):\")\n        print(f\"  Range: {risk_stats.get('min', 'N/A'):.3f} to {risk_stats.get('max', 'N/A'):.3f}\")\n        print(f\"  Mean: {risk_stats.get('mean', 'N/A'):.3f} Â± {risk_stats.get('std', 'N/A'):.3f}\")\n        print(f\"  Unique values: {risk_stats.get('unique_values', 'N/A')}\")\n    \n    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æçµæœ\n    if \"sequence_analysis\" in comprehensive_report and \"error\" not in comprehensive_report[\"sequence_analysis\"]:\n        seq_info = comprehensive_report[\"sequence_analysis\"]\n        length_analysis = seq_info.get(\"length_analysis\", {})\n        \n        print(\"\\nğŸ“Š SEQUENCE CHARACTERISTICS:\")\n        print(f\"  Total sequences: {length_analysis.get('total_sequences', 'N/A')}\")\n        print(f\"  Avg sequence length: {length_analysis.get('avg_length', 'N/A'):.1f}\")\n        print(f\"  Length range: {length_analysis.get('min_length', 'N/A')} to {length_analysis.get('max_length', 'N/A')}\")\n    \n    # CMIæ¯”è¼ƒçµæœ\n    if \"cmi_comparison\" in comprehensive_report:\n        comparison = comprehensive_report[\"cmi_comparison\"][\"comparison_analysis\"]\n        \n        print(\"\\nğŸ”„ CMI DATASET COMPARISON:\")\n        similarities = comparison.get(\"similarities\", [])\n        differences = comparison.get(\"differences\", [])\n        \n        print(\"  Similarities:\")\n        for sim in similarities[:3]:\n            print(f\"    â€¢ {sim}\")\n        \n        print(\"  Key Differences:\")\n        for diff in differences[:3]:\n            print(f\"    â€¢ {diff}\")\n    \n    print(f\"\\nğŸ’¾ Detailed results saved to: {output_path}\")\n    print(f\"ğŸ“ Analysis completed successfully!\")\n\n\nif __name__ == \"__main__\":\n    main()