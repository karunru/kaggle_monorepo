{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Very important Update**\n\nOld version code has a severe mistake, which will only use last fold model for submission. If you use this notebook before 7/22, please be aware and check the change in `Submit` section."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Import"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-14T12:13:50.832467Z",
     "iopub.status.busy": "2025-07-14T12:13:50.831913Z",
     "iopub.status.idle": "2025-07-14T12:13:50.837696Z",
     "shell.execute_reply": "2025-07-14T12:13:50.836956Z",
     "shell.execute_reply.started": "2025-07-14T12:13:50.83244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dataset\n\nUse tof raw data and split tof statistic data, gap=16 is best in my trials."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T12:13:26.425896Z",
     "iopub.status.busy": "2025-07-14T12:13:26.425258Z",
     "iopub.status.idle": "2025-07-14T12:13:26.438898Z",
     "shell.execute_reply": "2025-07-14T12:13:26.438175Z",
     "shell.execute_reply.started": "2025-07-14T12:13:26.425871Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[[\"acc_x\", \"acc_y\", \"acc_z\"]].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1 / 200):  # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i + 1]\n",
    "        if (\n",
    "            np.all(np.isnan(q_t))\n",
    "            or np.all(np.isclose(q_t, 0))\n",
    "            or np.all(np.isnan(q_t_plus_dt))\n",
    "            or np.all(np.isclose(q_t_plus_dt, 0))\n",
    "        ):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i + 1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0  # В случае недействительных кватернионов\n",
    "            pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T12:07:46.518427Z",
     "iopub.status.busy": "2025-07-14T12:07:46.517964Z",
     "iopub.status.idle": "2025-07-14T12:07:46.683071Z",
     "shell.execute_reply": "2025-07-14T12:07:46.682331Z",
     "shell.execute_reply.started": "2025-07-14T12:07:46.518405Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols + self.feature_cols)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.imu_engineered_features = [\n",
    "            \"acc_mag\",\n",
    "            \"rot_angle\",\n",
    "            \"acc_mag_jerk\",\n",
    "            \"rot_angle_vel\",\n",
    "            \"linear_acc_mag\",\n",
    "            \"linear_acc_mag_jerk\",\n",
    "            \"angular_vel_x\",\n",
    "            \"angular_vel_y\",\n",
    "            \"angular_vel_z\",\n",
    "            \"angular_distance\",\n",
    "        ]\n",
    "\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = [\"mean\", \"std\", \"min\", \"max\"]\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        imu_cols_base = [\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\"]\n",
    "        imu_cols_base.extend([c for c in columns if c.startswith(\"rot_\") and c not in [\"rot_angle\", \"rot_angle_vel\"]])\n",
    "        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n",
    "        self.thm_cols = [c for c in columns if c.startswith(\"thm_\")]\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = (\n",
    "            [\n",
    "                \"acc_x\",\n",
    "                \"acc_y\",\n",
    "                \"acc_z\",\n",
    "                \"rot_x\",\n",
    "                \"rot_y\",\n",
    "                \"rot_z\",\n",
    "                \"rot_w\",\n",
    "                \"sequence_id\",\n",
    "                \"subject\",\n",
    "                \"sequence_type\",\n",
    "                \"gesture\",\n",
    "                \"orientation\",\n",
    "            ]\n",
    "            + [c for c in columns if c.startswith(\"thm_\")]\n",
    "            + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        )\n",
    "        self.fold_cols = [\"subject\", \"sequence_type\", \"gesture\", \"orientation\"]\n",
    "\n",
    "    def generate_tof_feature_names(self):\n",
    "        features = []\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f\"tof_{i}_{stat}\")\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f\"tof{self.tof_mode}_{i}_region_{r}_{stat}\")\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f\"tof{mode}_{i}_region_{r}_{stat}\")\n",
    "        return features\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"] ** 2 + df[\"acc_y\"] ** 2 + df[\"acc_z\"] ** 2)\n",
    "        df[\"rot_angle\"] = 2 * np.arccos(df[\"rot_w\"].clip(-1, 1))\n",
    "        df[\"acc_mag_jerk\"] = df.groupby(\"sequence_id\")[\"acc_mag\"].diff().fillna(0)\n",
    "        df[\"rot_angle_vel\"] = df.groupby(\"sequence_id\")[\"rot_angle\"].diff().fillna(0)\n",
    "\n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby(\"sequence_id\"):\n",
    "            acc_data_group = group[[\"acc_x\", \"acc_y\", \"acc_z\"]]\n",
    "            rot_data_group = group[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(\n",
    "                pd.DataFrame(\n",
    "                    linear_accel_group, columns=[\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\"], index=group.index\n",
    "                )\n",
    "            )\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df[\"linear_acc_mag\"] = np.sqrt(df[\"linear_acc_x\"] ** 2 + df[\"linear_acc_y\"] ** 2 + df[\"linear_acc_z\"] ** 2)\n",
    "        df[\"linear_acc_mag_jerk\"] = df.groupby(\"sequence_id\")[\"linear_acc_mag\"].diff().fillna(0)\n",
    "\n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby(\"sequence_id\"):\n",
    "            rot_data_group = group[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(\n",
    "                pd.DataFrame(\n",
    "                    angular_vel_group, columns=[\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"], index=group.index\n",
    "                )\n",
    "            )\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "\n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby(\"sequence_id\"):\n",
    "            rot_data_group = group[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(\n",
    "                pd.DataFrame(angular_dist_group, columns=[\"angular_distance\"], index=group.index)\n",
    "            )\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update(\n",
    "                    {\n",
    "                        f\"tof_{i}_mean\": tof_data.mean(axis=1),\n",
    "                        f\"tof_{i}_std\": tof_data.std(axis=1),\n",
    "                        f\"tof_{i}_min\": tof_data.min(axis=1),\n",
    "                        f\"tof_{i}_max\": tof_data.max(axis=1),\n",
    "                    }\n",
    "                )\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r * region_size : (r + 1) * region_size]\n",
    "                        new_columns.update(\n",
    "                            {\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_mean\": region_data.mean(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_std\": region_data.std(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_min\": region_data.min(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_max\": region_data.max(axis=1),\n",
    "                            }\n",
    "                        )\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r * region_size : (r + 1) * region_size]\n",
    "                            new_columns.update(\n",
    "                                {\n",
    "                                    f\"tof{mode}_{i}_region_{r}_mean\": region_data.mean(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_std\": region_data.std(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_min\": region_data.min(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_max\": region_data.max(axis=1),\n",
    "                                }\n",
    "                            )\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        return df\n",
    "\n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        df[\"gesture_int\"] = self.le.fit_transform(df[\"gesture\"])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "\n",
    "        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n",
    "            print(\"Have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype=\"float32\")\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby(\"sequence_id\")\n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n",
    "        classes, lens = [], []\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype(\"float32\"))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype(\"float32\"))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype(\"float32\"))\n",
    "\n",
    "            classes.append(seq_df[\"gesture_int\"].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "\n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [\n",
    "                np.concatenate([imu, thm, tof], axis=1)\n",
    "                for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)\n",
    "            ]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols + self.thm_cols + self.tof_cols)\n",
    "            self.imu = x[..., : self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim : self.imu_dim + self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim + self.thm_dim : self.imu_dim + self.thm_dim + self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = (\n",
    "            F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        )\n",
    "        self.class_weight = torch.FloatTensor(\n",
    "            compute_class_weight(\"balanced\", classes=np.arange(len(self.le.classes_)), y=classes)\n",
    "        )\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array(\n",
    "                [\n",
    "                    [self.imu_nan_value] * len(self.imu_cols)\n",
    "                    + [self.thm_nan_value] * len(self.thm_cols)\n",
    "                    + [self.tof_nan_value] * len(self.tof_cols)\n",
    "                ]\n",
    "            ),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols,\n",
    "        )\n",
    "\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, : self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim : self.imu_dim + self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[\n",
    "                0, self.imu_dim + self.thm_dim : self.imu_dim + self.thm_dim + self.tof_dim\n",
    "            ].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return (\n",
    "            torch.full(imu.shape, self.imu_scaled_nan, device=imu.device),\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device),\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device),\n",
    "        )\n",
    "\n",
    "    def inference_process(self, sequence):\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n",
    "            df_seq[\"acc_mag\"] = np.sqrt(df_seq[\"acc_x\"] ** 2 + df_seq[\"acc_y\"] ** 2 + df_seq[\"acc_z\"] ** 2)\n",
    "            df_seq[\"rot_angle\"] = 2 * np.arccos(df_seq[\"rot_w\"].clip(-1, 1))\n",
    "            df_seq[\"acc_mag_jerk\"] = df_seq[\"acc_mag\"].diff().fillna(0)\n",
    "            df_seq[\"rot_angle_vel\"] = df_seq[\"rot_angle\"].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in [\"acc_x\", \"acc_y\", \"acc_z\", \"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[[\"acc_x\", \"acc_y\", \"acc_z\"]], df_seq[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]]\n",
    "                )\n",
    "                df_seq[[\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\"]] = linear_accel\n",
    "            else:\n",
    "                df_seq[\"linear_acc_x\"] = df_seq.get(\"acc_x\", 0)\n",
    "                df_seq[\"linear_acc_y\"] = df_seq.get(\"acc_y\", 0)\n",
    "                df_seq[\"linear_acc_z\"] = df_seq.get(\"acc_z\", 0)\n",
    "            df_seq[\"linear_acc_mag\"] = np.sqrt(\n",
    "                df_seq[\"linear_acc_x\"] ** 2 + df_seq[\"linear_acc_y\"] ** 2 + df_seq[\"linear_acc_z\"] ** 2\n",
    "            )\n",
    "            df_seq[\"linear_acc_mag_jerk\"] = df_seq[\"linear_acc_mag\"].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in [\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]])\n",
    "                df_seq[[\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]] = angular_vel\n",
    "            else:\n",
    "                df_seq[[\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]] = 0\n",
    "            if all(col in df_seq.columns for col in [\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]):\n",
    "                df_seq[\"angular_distance\"] = calculate_angular_distance(df_seq[[\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]])\n",
    "            else:\n",
    "                df_seq[\"angular_distance\"] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update(\n",
    "                    {\n",
    "                        f\"tof_{i}_mean\": tof_data.mean(axis=1),\n",
    "                        f\"tof_{i}_std\": tof_data.std(axis=1),\n",
    "                        f\"tof_{i}_min\": tof_data.min(axis=1),\n",
    "                        f\"tof_{i}_max\": tof_data.max(axis=1),\n",
    "                    }\n",
    "                )\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r * region_size : (r + 1) * region_size]\n",
    "                        new_columns.update(\n",
    "                            {\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_mean\": region_data.mean(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_std\": region_data.std(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_min\": region_data.min(axis=1),\n",
    "                                f\"tof{self.tof_mode}_{i}_region_{r}_max\": region_data.max(axis=1),\n",
    "                            }\n",
    "                        )\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r * region_size : (r + 1) * region_size]\n",
    "                            new_columns.update(\n",
    "                                {\n",
    "                                    f\"tof{mode}_{i}_region_{r}_mean\": region_data.mean(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_std\": region_data.std(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_min\": region_data.min(axis=1),\n",
    "                                    f\"tof{mode}_{i}_region_{r}_max\": region_data.max(axis=1),\n",
    "                                }\n",
    "                            )\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype(\"float32\")\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype(\"float32\")\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype(\"float32\")\n",
    "\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., : self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim : self.imu_dim + self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim + self.thm_dim : self.imu_dim + self.thm_dim + self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype=\"float32\")\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., : self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim : self.imu_dim + self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim + self.thm_dim : self.imu_dim + self.thm_dim + self.tof_dim]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(imu).float().unsqueeze(0),\n",
    "            torch.from_numpy(thm).float().unsqueeze(0),\n",
    "            torch.from_numpy(tof).float().unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "\n",
    "class CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.folds = list(\n",
    "            self.skf.split(np.arange(len(self.full_dataset)), np.array(self.full_dataset.dataset_indices))\n",
    "        )\n",
    "\n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds:\n",
    "            return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = dict.fromkeys(self.class_names, 0)\n",
    "            if subset is None:\n",
    "                return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "\n",
    "        print(\"\\n交叉验证折叠统计:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model\n\nUse bert instead of simple attention layers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-07-14T12:07:46.6839Z",
     "iopub.status.busy": "2025-07-14T12:07:46.683694Z",
     "iopub.status.idle": "2025-07-14T12:07:46.702397Z",
     "shell.execute_reply": "2025-07-14T12:07:46.701577Z",
     "shell.execute_reply.started": "2025-07-14T12:07:46.683885Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)  # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)  # -> (B, C, 1)\n",
    "        return x * se\n",
    "\n",
    "\n",
    "class ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd=1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = SEBlock(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=False), nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)  # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)  # (B, out, L)\n",
    "        out = out + identity\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class CMIModel(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            self.residual_se_cnn_block(\n",
    "                imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]\n",
    "            ),\n",
    "            self.residual_se_cnn_block(\n",
    "                kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.thm_branch = nn.Sequential(\n",
    "            nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"thm1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm1_dropout\"]),\n",
    "            nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm2_dropout\"]),\n",
    "        )\n",
    "\n",
    "        self.tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"tof1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof1_dropout\"]),\n",
    "            nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof2_dropout\"]),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n",
    "        self.bert = BertModel(\n",
    "            BertConfig(\n",
    "                hidden_size=kwargs[\"feat_dim\"],\n",
    "                num_hidden_layers=kwargs[\"bert_layers\"],\n",
    "                num_attention_heads=kwargs[\"bert_heads\"],\n",
    "                intermediate_size=kwargs[\"feat_dim\"] * 4,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls1_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls2_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls2_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls2_channels\"], n_classes),\n",
    "        )\n",
    "\n",
    "    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            ResNetSEBlock(in_channels, out_channels, wd=wd),\n",
    "            nn.MaxPool1d(pool_size),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, imu, thm, tof):\n",
    "        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n",
    "\n",
    "        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n",
    "        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)  # (B,1,H)\n",
    "        bert_input = torch.cat([cls_token, bert_input], dim=1)  # (B,T+1,H)\n",
    "        outputs = self.bert(inputs_embeds=bert_input)\n",
    "        pred_cls = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return self.classifier(pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Settings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:07:46.704321Z",
     "iopub.status.busy": "2025-07-14T12:07:46.703831Z",
     "iopub.status.idle": "2025-07-14T12:07:49.913764Z",
     "shell.execute_reply": "2025-07-14T12:07:49.913152Z",
     "shell.execute_reply.started": "2025-07-14T12:07:46.704296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "deterministic = kagglehub.package_import(\"wasupandceacar/deterministic\").deterministic\n",
    "deterministic.init_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:07:49.91526Z",
     "iopub.status.busy": "2025-07-14T12:07:49.914703Z",
     "iopub.status.idle": "2025-07-14T12:09:25.261519Z",
     "shell.execute_reply": "2025-07-14T12:09:25.260683Z",
     "shell.execute_reply.started": "2025-07-14T12:07:49.915233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 95,\n",
    "        \"scaler_function\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": True,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "    }\n",
    "    dataset = CMIFoldDataset(\n",
    "        universe_csv_path, dataset_config, n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset\n",
    "    )\n",
    "    dataset.print_fold_stats()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "\n",
    "dataset = init_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:11:11.563727Z",
     "iopub.status.busy": "2025-07-14T12:11:11.563018Z",
     "iopub.status.idle": "2025-07-14T12:11:21.604301Z",
     "shell.execute_reply": "2025-07-14T12:11:21.603718Z",
     "shell.execute_reply.started": "2025-07-14T12:11:11.563702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_function = CMIModel\n",
    "model_args = {\n",
    "    \"feat_dim\": 500,\n",
    "    \"imu1_channels\": 219,\n",
    "    \"imu1_dropout\": 0.2946731587132302,\n",
    "    \"imu2_dropout\": 0.2697745571929592,\n",
    "    \"imu1_weight_decay\": 0.0014824054650601245,\n",
    "    \"imu2_weight_decay\": 0.002742543773142381,\n",
    "    \"imu1_layers\": 0,\n",
    "    \"imu2_layers\": 0,\n",
    "    \"thm1_channels\": 82,\n",
    "    \"thm1_dropout\": 0.2641274454844602,\n",
    "    \"thm2_dropout\": 0.302896343020985,\n",
    "    \"tof1_channels\": 82,\n",
    "    \"tof1_dropout\": 0.2641274454844602,\n",
    "    \"tof2_dropout\": 0.3028963430209852,\n",
    "    \"bert_layers\": 8,\n",
    "    \"bert_heads\": 10,\n",
    "    \"cls1_channels\": 937,\n",
    "    \"cls2_channels\": 303,\n",
    "    \"cls1_dropout\": 0.2281834512100508,\n",
    "    \"cls2_dropout\": 0.22502521933558461,\n",
    "}\n",
    "model_args.update(\n",
    "    {\n",
    "        \"imu_dim\": dataset.full_dataset.imu_dim,\n",
    "        \"thm_dim\": dataset.full_dataset.thm_dim,\n",
    "        \"tof_dim\": dataset.full_dataset.tof_dim,\n",
    "        \"n_classes\": dataset.full_dataset.class_num,\n",
    "    }\n",
    ")\n",
    "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
    "\n",
    "model_dicts = [\n",
    "    {\n",
    "        \"model_function\": model_function,\n",
    "        \"model_args\": model_args,\n",
    "        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n",
    "    }\n",
    "    for fold in range(n_folds)\n",
    "]\n",
    "\n",
    "models = list()\n",
    "for model_dict in model_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Valid\n\nAccording to competition test datasets, valid both on full data and Imu only data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:11:37.558796Z",
     "iopub.status.busy": "2025-07-14T12:11:37.558091Z",
     "iopub.status.idle": "2025-07-14T12:12:40.332618Z",
     "shell.execute_reply": "2025-07-14T12:12:40.331796Z",
     "shell.execute_reply.started": "2025-07-14T12:11:37.558753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metric_package = kagglehub.package_import(\"wasupandceacar/cmi-metric\")\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "\n",
    "def to_cuda(*tensors):\n",
    "    return [tensor.to(CUDA0) for tensor in tensors]\n",
    "\n",
    "\n",
    "def predict_valid(model, imu, thm, tof):\n",
    "    pred = model(imu, thm, tof)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for imu, thm, tof, y in valid_bar:\n",
    "            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = predict_valid(model, imu, thm, tof)\n",
    "            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = model(imu, thm, tof)\n",
    "            imu_only_metric.add(\n",
    "                dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()]\n",
    "            )\n",
    "\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    valid_loader = get_fold_dataset(dataset, fold)\n",
    "    valid_bar = tqdm(valid_loader, desc=\"Valid\", position=0, leave=False)\n",
    "    valid(model, valid_bar)\n",
    "\n",
    "print(f\"\"\"\n",
    "Normal score: {metric.score()}\n",
    "IMU only score: {imu_only_metric.score()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Submit"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:14:01.43082Z",
     "iopub.status.busy": "2025-07-14T12:14:01.430018Z",
     "iopub.status.idle": "2025-07-14T12:14:02.99601Z",
     "shell.execute_reply": "2025-07-14T12:14:02.995078Z",
     "shell.execute_reply.started": "2025-07-14T12:14:01.430794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def avg_predict(models, imu, thm, tof):\n",
    "    outputs = []\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        for model in models:\n",
    "            logits = model(imu, thm, tof)\n",
    "            outputs.append(logits)  # indent was wrong in older version\n",
    "    return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        logits = avg_predict(models, imu, thm, tof)\n",
    "    return dataset.le.classes_[logits.argmax(dim=1).cpu()]\n",
    "\n",
    "\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\",\n",
    "            \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\",\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 398856,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}